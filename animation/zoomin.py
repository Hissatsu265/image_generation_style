# # !pip install opencv-python mediapipe pillow numpy moviepy

# import cv2
# import numpy as np
# from PIL import Image
# import os
# import mediapipe as mp
# import random
# import math

# class VideoFaceZoom:
#     def __init__(self, input_video_path, output_video_path):
#         self.input_path = input_video_path
#         self.output_path = output_video_path

#         # Kh·ªüi t·∫°o MediaPipe Face Detection
#         self.mp_face_detection = mp.solutions.face_detection
#         self.mp_drawing = mp.solutions.drawing_utils
#         self.face_detection = self.mp_face_detection.FaceDetection(
#             model_selection=0, min_detection_confidence=0.5
#         )

#         # ƒê·ªçc video
#         self.cap = cv2.VideoCapture(input_video_path)
#         self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
#         self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#         self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#         self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))

#         # Thi·∫øt l·∫≠p video writer
#         fourcc = cv2.VideoWriter_fourcc(*'mp4v')
#         self.out = cv2.VideoWriter(output_video_path, fourcc, self.fps, (self.width, self.height))

#     def detect_faces(self, frame):
#         rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#         results = self.face_detection.process(rgb_frame)

#         faces = []
#         if results.detections:
#             for detection in results.detections:
#                 bbox = detection.location_data.relative_bounding_box

#                 x = int(bbox.xmin * self.width)
#                 y = int(bbox.ymin * self.height)
#                 w = int(bbox.width * self.width)
#                 h = int(bbox.height * self.height)
#                 padding_left = 0.3    
#                 padding_right = 0.3   
#                 padding_top = 0.5     
#                 padding_bottom = 0.2  
#                 if self.height > self.width:
#                     padding_left = padding_right = padding_top = padding_bottom = 0.1
#                     # print("======H>W===")
                     

#                 x_expand_left = int(w * padding_left)
#                 x_expand_right = int(w * padding_right)
#                 y_expand_top = int(h * padding_top)
#                 y_expand_bottom = int(h * padding_bottom)

#                 x = max(0, x - x_expand_left)
#                 y = max(0, y - y_expand_top)
#                 w = min(self.width - x, w + x_expand_left + x_expand_right)
#                 h = min(self.height - y, h + y_expand_top + y_expand_bottom)

#                 faces.append((x, y, x + w, y + h))

#         return faces

#     def get_largest_face(self, faces):
#         if not faces:
#             return None
#         largest_face = max(faces, key=lambda face: (face[2] - face[0]) * (face[3] - face[1]))
#         return largest_face

#     # def calculate_zoom_region(self, face_bbox, zoom_factor=1.5):
#     #     x1, y1, x2, y2 = face_bbox
#     #     face_center_x = (x1 + x2) // 2
#     #     face_center_y = (y1 + y2) // 2

#     #     zoom_width = int(self.width / zoom_factor)
#     #     zoom_height = int(self.height / zoom_factor)

#     #     zoom_x1 = max(0, face_center_x - zoom_width // 2)
#     #     zoom_y1 = max(0, face_center_y - zoom_height // 2)
#     #     zoom_x2 = min(self.width, zoom_x1 + zoom_width)
#     #     zoom_y2 = min(self.height, zoom_y1 + zoom_height)

#     #     if zoom_x2 - zoom_x1 < zoom_width:
#     #         zoom_x1 = max(0, zoom_x2 - zoom_width)
#     #     if zoom_y2 - zoom_y1 < zoom_height:
#     #         zoom_y1 = max(0, zoom_y2 - zoom_height)

#     #     return (zoom_x1, zoom_y1, zoom_x2, zoom_y2)
#     def calculate_zoom_region(self, face_bbox, zoom_factor=1.5):
#         x1, y1, x2, y2 = face_bbox
#         face_center_x = (x1 + x2) // 2
#         face_center_y = (y1 + y2) // 2

#         zoom_width = int(self.width / zoom_factor)
#         zoom_height = int(self.height / zoom_factor)

#         # ‚ö†Ô∏è ƒêi·ªÅu ch·ªânh ƒë·ªÉ v√πng zoom l·ªách xu·ªëng d∆∞·ªõi n·∫øu l√† video d·ªçc
#         if self.height > self.width:
#             face_center_y += int(0.1 * self.height)  # ƒë·∫©y v√πng zoom xu·ªëng 10% chi·ªÅu cao

#         zoom_x1 = max(0, face_center_x - zoom_width // 2)
#         zoom_y1 = max(0, face_center_y - zoom_height // 2)
#         zoom_x2 = min(self.width, zoom_x1 + zoom_width)
#         zoom_y2 = min(self.height, zoom_y1 + zoom_height)

#         if zoom_x2 - zoom_x1 < zoom_width:
#             zoom_x1 = max(0, zoom_x2 - zoom_width)
#         if zoom_y2 - zoom_y1 < zoom_height:
#             zoom_y1 = max(0, zoom_y2 - zoom_height)

#         return (zoom_x1, zoom_y1, zoom_x2, zoom_y2)

#     def smooth_transition(self, current_region, target_region, alpha=0.1):
#         if current_region is None:
#             return target_region

#         smooth_region = []
#         for i in range(4):
#             smooth_val = int(current_region[i] * (1 - alpha) + target_region[i] * alpha)
#             smooth_region.append(smooth_val)

#         return tuple(smooth_region)

#     def apply_shake_effect(self, zoom_region, shake_intensity=5):
        
#         if zoom_region is None:
#             return zoom_region
            
#         x1, y1, x2, y2 = zoom_region
        
#         # T·∫°o ƒë·ªô l·ªách ng·∫´u nhi√™n
#         shake_x = random.randint(-shake_intensity, shake_intensity)
#         shake_y = random.randint(-shake_intensity, shake_intensity)
        
#         # √Åp d·ª•ng shake nh∆∞ng ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t bi√™n
#         new_x1 = max(0, min(self.width - (x2 - x1), x1 + shake_x))
#         new_y1 = max(0, min(self.height - (y2 - y1), y1 + shake_y))
#         new_x2 = new_x1 + (x2 - x1)
#         new_y2 = new_y1 + (y2 - y1)
        
#         return (new_x1, new_y1, new_x2, new_y2)

#     def calculate_gradual_zoom_factor(self, current_frame, start_frame, end_frame, start_zoom=1.0, end_zoom=1.5):
#         """
#         T√≠nh zoom factor cho zoom t·ª´ t·ª´
#         """
#         if current_frame < start_frame:
#             return start_zoom
#         elif current_frame >= end_frame:
#             return end_zoom
#         else:
#             # T√≠nh to√°n zoom factor theo th·ªùi gian (easing)
#             progress = (current_frame - start_frame) / (end_frame - start_frame)
#             # S·ª≠ d·ª•ng easing function ƒë·ªÉ smooth h∆°n
#             eased_progress = 1 - math.cos(progress * math.pi / 2)  # ease-out sine
#             return start_zoom + (end_zoom - start_zoom) * eased_progress

#     def apply_zoom_effect(self, frame, zoom_region, zoom_factor=1.5):
#         x1, y1, x2, y2 = zoom_region
#         cropped = frame[y1:y2, x1:x2]
#         zoomed = cv2.resize(cropped, (self.width, self.height), interpolation=cv2.INTER_CUBIC)
#         return zoomed

#     def process_video(self, zoom_start_frame=None, zoom_duration_frames=None, zoom_factor=1.5, 
#                      zoom_type="instant", gradual_start_frame=None, gradual_end_frame=None, 
#                      gradual_hold_frames=None, enable_shake=False, shake_intensity=3, shake_start_delay=0.5):

        
#         frame_count = 0
#         zoom_region = None
#         target_face = None
#         shake_start_frame = None
#         shake_end_frame = None
#         shake_duration_frames = int(0.5 * self.fps)  # Shake ch·ªâ k√©o d√†i 0.5 gi√¢y
        
#         if zoom_type == "instant":
#             print(f"Instant zoom t·ª´ frame {zoom_start_frame} ƒë·∫øn {zoom_start_frame + zoom_duration_frames}")
#             if enable_shake:
#                 shake_start_frame = zoom_start_frame + int(shake_start_delay * self.fps)
#                 shake_end_frame = shake_start_frame + shake_duration_frames
#                 print(f"Shake effect t·ª´ frame {shake_start_frame} ƒë·∫øn {shake_end_frame}")
#         else:
#             gradual_total_end = gradual_end_frame + (gradual_hold_frames or 0)
#             print(f"Gradual zoom t·ª´ frame {gradual_start_frame} ƒë·∫øn {gradual_end_frame}")
#             print(f"Gi·ªØ m·ª©c zoom ƒë·∫øn frame {gradual_total_end}")
#             if enable_shake:
#                 shake_start_frame = gradual_end_frame + int(shake_start_delay * self.fps)
#                 shake_end_frame = shake_start_frame + shake_duration_frames
#                 print(f"Shake effect t·ª´ frame {shake_start_frame} ƒë·∫øn {shake_end_frame}")

#         while True:
#             ret, frame = self.cap.read()
#             if not ret:
#                 break

#             faces = self.detect_faces(frame)
#             current_zoom_factor = 1.0
#             should_zoom = False

#             # X√°c ƒë·ªãnh c√≥ n√™n zoom kh√¥ng v√† zoom factor
#             if zoom_type == "instant":
#                 if zoom_start_frame <= frame_count < zoom_start_frame + zoom_duration_frames:
#                     should_zoom = True
#                     current_zoom_factor = zoom_factor
#             elif zoom_type == "gradual":
#                 if gradual_start_frame <= frame_count <= gradual_end_frame:
#                     # ƒêang trong giai ƒëo·∫°n zoom t·ª´ t·ª´
#                     should_zoom = True
#                     current_zoom_factor = self.calculate_gradual_zoom_factor(
#                         frame_count, gradual_start_frame, gradual_end_frame, 1.0, zoom_factor
#                     )
#                 elif gradual_hold_frames and gradual_end_frame < frame_count <= gradual_end_frame + gradual_hold_frames:
#                     # ƒêang trong giai ƒëo·∫°n gi·ªØ m·ª©c zoom t·ªëi ƒëa
#                     should_zoom = True
#                     current_zoom_factor = zoom_factor

#             # X·ª≠ l√Ω zoom
#             if should_zoom and current_zoom_factor > 1.0:
#                 if faces:
#                     current_largest = self.get_largest_face(faces)

#                     if target_face is None:
#                         target_face = current_largest
#                     else:
#                         target_area = (target_face[2] - target_face[0]) * (target_face[3] - target_face[1])
#                         current_area = (current_largest[2] - current_largest[0]) * (current_largest[3] - current_largest[1])

#                         if current_area > target_area * 1.2:
#                             target_face = current_largest

#                     target_zoom_region = self.calculate_zoom_region(target_face, current_zoom_factor)
#                     zoom_region = self.smooth_transition(zoom_region, target_zoom_region, alpha=0.15)

#                     # √Åp d·ª•ng shake effect ch·ªâ trong kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh
#                     if (enable_shake and shake_start_frame and shake_end_frame and 
#                         shake_start_frame <= frame_count < shake_end_frame):
#                         zoom_region = self.apply_shake_effect(zoom_region, shake_intensity)

#                     frame = self.apply_zoom_effect(frame, zoom_region, current_zoom_factor)

#                 elif zoom_region is not None:
#                     # √Åp d·ª•ng shake effect ch·ªâ trong kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh
#                     if (enable_shake and shake_start_frame and shake_end_frame and 
#                         shake_start_frame <= frame_count < shake_end_frame):
#                         zoom_region = self.apply_shake_effect(zoom_region, shake_intensity)
                    
#                     frame = self.apply_zoom_effect(frame, zoom_region, current_zoom_factor)
#             else:
#                 # Reset khi kh√¥ng zoom
#                 if not should_zoom:
#                     target_face = None
#                     zoom_region = None

#             self.out.write(frame)
#             frame_count += 1
            
#             if frame_count % 30 == 0:
#                 print(f"ƒê√£ x·ª≠ l√Ω: {frame_count}/{self.total_frames} frames")

#         print("Ho√†n th√†nh x·ª≠ l√Ω video!")

#     def release(self):
#         self.cap.release()
#         self.out.release()

# def create_face_zoom_video(input_video, output_video, zoom_type="instant", **kwargs):
    
    
#     processor = VideoFaceZoom(input_video, output_video)
    
#     # Thi·∫øt l·∫≠p parameters
#     zoom_factor = kwargs.get('zoom_factor', 1.8)
#     enable_shake = kwargs.get('enable_shake', False)
#     shake_intensity = kwargs.get('shake_intensity', 3)
#     shake_start_delay = kwargs.get('shake_start_delay', 0.5)
    
#     try:
#         if zoom_type == "instant":
#             zoom_start_time = kwargs.get('zoom_start_time', 0)
#             zoom_duration = kwargs.get('zoom_duration', 2)
            
#             zoom_start_frame = int(zoom_start_time * processor.fps)
#             zoom_duration_frames = int(zoom_duration * processor.fps)
            
#             processor.process_video(
#                 zoom_start_frame=zoom_start_frame,
#                 zoom_duration_frames=zoom_duration_frames,
#                 zoom_factor=zoom_factor,
#                 zoom_type="instant",
#                 enable_shake=enable_shake,
#                 shake_intensity=shake_intensity,
#                 shake_start_delay=shake_start_delay
#             )
            
#         elif zoom_type == "gradual":
#             gradual_start_time = kwargs.get('gradual_start_time', 0)
#             gradual_end_time = kwargs.get('gradual_end_time', 3)
#             hold_duration = kwargs.get('hold_duration', 2)  # M·∫∑c ƒë·ªãnh gi·ªØ 2 gi√¢y
            
#             gradual_start_frame = int(gradual_start_time * processor.fps)
#             gradual_end_frame = int(gradual_end_time * processor.fps)
#             gradual_hold_frames = int(hold_duration * processor.fps)
            
#             processor.process_video(
#                 zoom_factor=zoom_factor,
#                 zoom_type="gradual",
#                 gradual_start_frame=gradual_start_frame,
#                 gradual_end_frame=gradual_end_frame,
#                 gradual_hold_frames=gradual_hold_frames,
#                 enable_shake=enable_shake,
#                 shake_intensity=shake_intensity,
#                 shake_start_delay=shake_start_delay
#             )
        
#     finally:
#         processor.release()

# from moviepy.editor import VideoFileClip

# def replace_audio(video_path, audio_source_path, output_path):
#     video = VideoFileClip(video_path)
#     new_audio = VideoFileClip(audio_source_path).audio
#     video_with_new_audio = video.set_audio(new_audio)
#     video_with_new_audio.write_videofile(output_path, codec="libx264", audio_codec="aac")
#     return output_path


# # if __name__ == "__main__":
# #     input_video_path = "/workspace/marketing-video-ai/e06b2a6c_clip_1_cut_10.3s.mp4"
    
# #     # V√≠ d·ª• 1: Zoom ƒë·ªôt ng·ªôt v·ªõi hi·ªáu ·ª©ng rung
# #     print("=== T·∫°o zoom ƒë·ªôt ng·ªôt v·ªõi shake effect ===")
# #     create_face_zoom_video(
# #         input_video=input_video_path,
# #         output_video="output_instant_zoom_shake.mp4",
# #         zoom_type="instant",
# #         zoom_start_time=1,
# #         zoom_duration=4,
# #         zoom_factor=1.8,
# #         enable_shake=True,
# #         shake_intensity=1,
# #         shake_start_delay=0.3
# #     )
    
# #     #V√≠ d·ª• 2: Zoom t·ª´ t·ª´ v·ªõi gi·ªØ m·ª©c zoom
# #     print("=== T·∫°o zoom t·ª´ t·ª´ v·ªõi gi·ªØ m·ª©c zoom ===")
# #     create_face_zoom_video(
# #         input_video=input_video_path,
# #         output_video="output_gradual_zoom.mp4",
# #         zoom_type="gradual",
# #         gradual_start_time=1,
# #         gradual_end_time=1.5,  # Zoom t·ª´ gi√¢y 1 ƒë·∫øn gi√¢y 3
# #         hold_duration=2,     # Gi·ªØ m·ª©c zoom trong 2 gi√¢y (ƒë·∫øn gi√¢y 5)
# #         zoom_factor=1.3
# #     )
    
# #     # V√≠ d·ª• 3: Zoom t·ª´ t·ª´ v·ªõi shake effect (ch·ªâ 0.5 gi√¢y)
# #     print("=== T·∫°o zoom t·ª´ t·ª´ v·ªõi shake effect ===")
# #     create_face_zoom_video(
# #         input_video=input_video_path,
# #         output_video="output_gradual_zoom_shake.mp4",
# #         zoom_type="gradual",
# #         gradual_start_time=1,
# #         gradual_end_time=3,    # Zoom t·ª´ gi√¢y 1 ƒë·∫øn gi√¢y 3
# #         hold_duration=2,       # Gi·ªØ m·ª©c zoom ƒë·∫øn gi√¢y 5
# #         zoom_factor=1.8,
# #         enable_shake=True,
# #         shake_intensity=3,
# #         shake_start_delay=0.2  # Shake b·∫Øt ƒë·∫ßu sau 0.2s khi zoom xong, k√©o d√†i 0.5s
# #     )
    
#     # Thay audio
#     # output = replace_audio("output_gradual_zoom.mp4", "/content/25_7padd.mp4", "final_output.mp4")
#     # print("Video cu·ªëi c√πng:", output)
import cv2
import numpy as np
from PIL import Image
import os
# import mediapipe as mp
import random
import math
import time
from moviepy.editor import VideoFileClip

def wait_for_file_ready(file_path, min_size_mb=0.1, max_wait_time=60, check_interval=1):
    """
    Ki·ªÉm tra file ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng
    
    Args:
        file_path: ƒë∆∞·ªùng d·∫´n file c·∫ßn ki·ªÉm tra
        min_size_mb: k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu c·ªßa file (MB)
        max_wait_time: th·ªùi gian ch·ªù t·ªëi ƒëa (gi√¢y)
        check_interval: kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn ki·ªÉm tra (gi√¢y)
    
    Returns:
        bool: True n·∫øu file s·∫µn s√†ng, False n·∫øu timeout
    """
    print(f"ƒêang ki·ªÉm tra file: {file_path}")
    start_time = time.time()
    min_size_bytes = min_size_mb * 1024 * 1024
    last_size = 0
    stable_count = 0
    
    while time.time() - start_time < max_wait_time:
        # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(file_path):
            print(f"File ch∆∞a t·ªìn t·∫°i. Ch·ªù {check_interval}s...")
            time.sleep(check_interval)
            continue
        
        try:
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
            current_size = os.path.getsize(file_path)
            print(f"K√≠ch th∆∞·ªõc file hi·ªán t·∫°i: {current_size / (1024*1024):.2f} MB")
            
            # Ki·ªÉm tra file c√≥ ƒë·ªß k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu kh√¥ng
            if current_size < min_size_bytes:
                print(f"File ch∆∞a ƒë·ªß k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu ({min_size_mb} MB). Ch·ªù...")
                time.sleep(check_interval)
                continue
            
            # Ki·ªÉm tra file c√≥ ƒëang ƒë∆∞·ª£c ghi kh√¥ng (k√≠ch th∆∞·ªõc ·ªïn ƒë·ªãnh)
            if current_size == last_size:
                stable_count += 1
                if stable_count >= 3:  # File ·ªïn ƒë·ªãnh trong 3 l·∫ßn ki·ªÉm tra
                    print("‚úÖ File ·ªïn ƒë·ªãnh, ti·∫øn h√†nh ki·ªÉm tra t√≠nh to√†n v·∫πn...")
                    
                    # Ki·ªÉm tra file c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng
                    try:
                        cap = cv2.VideoCapture(file_path)
                        if cap.isOpened():
                            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                            fps = cap.get(cv2.CAP_PROP_FPS)
                            cap.release()
                            
                            if frame_count > 0 and fps > 0:
                                print(f"‚úÖ File h·ª£p l·ªá - Frames: {frame_count}, FPS: {fps}")
                                return True
                            else:
                                print("‚ùå File video kh√¥ng h·ª£p l·ªá")
                        else:
                            print("‚ùå Kh√¥ng th·ªÉ m·ªü file video")
                    except Exception as e:
                        print(f"‚ùå L·ªói khi ki·ªÉm tra file: {e}")
                    
                    time.sleep(check_interval)
            else:
                stable_count = 0
                last_size = current_size
                print(f"File ƒëang thay ƒë·ªïi k√≠ch th∆∞·ªõc...")
                time.sleep(check_interval)
                
        except Exception as e:
            print(f"L·ªói khi ki·ªÉm tra file: {e}")
            time.sleep(check_interval)
    
    print(f"‚ùå Timeout sau {max_wait_time}s")
    return False

def safe_video_processing(input_file, output_file, processing_func, *args, **kwargs):
    """
    X·ª≠ l√Ω video an to√†n v·ªõi ki·ªÉm tra file
    
    Args:
        input_file: file input
        output_file: file output  
        processing_func: h√†m x·ª≠ l√Ω video
        *args, **kwargs: tham s·ªë cho h√†m x·ª≠ l√Ω
    
    Returns:
        bool: True n·∫øu th√†nh c√¥ng
    """
    print(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω: {input_file} -> {output_file}")
    
    # Ki·ªÉm tra file input
    if not wait_for_file_ready(input_file):
        print(f"‚ùå File input kh√¥ng s·∫µn s√†ng: {input_file}")
        return False
    
    # X√≥a file output c≈© n·∫øu t·ªìn t·∫°i
    if os.path.exists(output_file):
        try:
            os.remove(output_file)
            print(f"üóëÔ∏è ƒê√£ x√≥a file c≈©: {output_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a file c≈©: {e}")
    
    # Th·ª±c hi·ªán x·ª≠ l√Ω
    try:
        result = processing_func(*args, **kwargs)
        
        # Ki·ªÉm tra file output ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng
        if wait_for_file_ready(output_file, min_size_mb=0.5):
            print(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng: {output_file}")
            return True
        else:
            print(f"‚ùå File output kh√¥ng ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng: {output_file}")
            return False
            
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
        return False

# Wrapper function cho create_face_zoom_video
def safe_create_face_zoom_video(input_video, output_video, **kwargs):
    """
    Version an to√†n c·ªßa create_face_zoom_video
    """
    return safe_video_processing(
        input_video, 
        output_video,
        create_face_zoom_video,
        input_video=input_video,
        output_video=output_video,
        **kwargs
    )

# C·∫≠p nh·∫≠t l·∫°i class VideoFaceZoom ƒë·ªÉ ƒë·∫£m b·∫£o gi·∫£i ph√≥ng t√†i nguy√™n
class VideoFaceZoom:
    def __init__(self, input_video_path, output_video_path):
        self.input_path = input_video_path
        self.output_path = output_video_path

        # Kh·ªüi t·∫°o MediaPipe Face Detection
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=0, min_detection_confidence=0.5
        )

        # ƒê·ªçc video
        self.cap = cv2.VideoCapture(input_video_path)
        if not self.cap.isOpened():
            raise ValueError(f"Kh√¥ng th·ªÉ m·ªü video: {input_video_path}")
            
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Thi·∫øt l·∫≠p video writer
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        self.out = cv2.VideoWriter(output_video_path, fourcc, self.fps, (self.width, self.height))
        
        if not self.out.isOpened():
            self.cap.release()
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫°o video writer: {output_video_path}")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

    def release(self):
        """Gi·∫£i ph√≥ng t√†i nguy√™n"""
        if hasattr(self, 'cap') and self.cap.isOpened():
            self.cap.release()
        if hasattr(self, 'out') and self.out.isOpened():
            self.out.release()
        print(f"‚úÖ ƒê√£ gi·∫£i ph√≥ng t√†i nguy√™n cho: {self.output_path}")

    # ... (gi·ªØ nguy√™n c√°c ph∆∞∆°ng th·ª©c kh√°c nh∆∞ detect_faces, get_largest_face, etc.)
    
    def detect_faces(self, frame):
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb_frame)

        faces = []
        if results.detections:
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box

                x = int(bbox.xmin * self.width)
                y = int(bbox.ymin * self.height)
                w = int(bbox.width * self.width)
                h = int(bbox.height * self.height)
                padding_left = 0.3    
                padding_right = 0.3   
                padding_top = 0.5     
                padding_bottom = 0.2  
                if self.height > self.width:
                    padding_left = padding_right = padding_top = padding_bottom = 0.1

                x_expand_left = int(w * padding_left)
                x_expand_right = int(w * padding_right)
                y_expand_top = int(h * padding_top)
                y_expand_bottom = int(h * padding_bottom)

                x = max(0, x - x_expand_left)
                y = max(0, y - y_expand_top)
                w = min(self.width - x, w + x_expand_left + x_expand_right)
                h = min(self.height - y, h + y_expand_top + y_expand_bottom)

                faces.append((x, y, x + w, y + h))

        return faces

    def get_largest_face(self, faces):
        if not faces:
            return None
        largest_face = max(faces, key=lambda face: (face[2] - face[0]) * (face[3] - face[1]))
        return largest_face

    def calculate_zoom_region(self, face_bbox, zoom_factor=1.5):
        x1, y1, x2, y2 = face_bbox
        face_center_x = (x1 + x2) // 2
        face_center_y = (y1 + y2) // 2

        zoom_width = int(self.width / zoom_factor)
        zoom_height = int(self.height / zoom_factor)

        if self.height > self.width:
            face_center_y += int(0.1 * self.height)

        zoom_x1 = max(0, face_center_x - zoom_width // 2)
        zoom_y1 = max(0, face_center_y - zoom_height // 2)
        zoom_x2 = min(self.width, zoom_x1 + zoom_width)
        zoom_y2 = min(self.height, zoom_y1 + zoom_height)

        if zoom_x2 - zoom_x1 < zoom_width:
            zoom_x1 = max(0, zoom_x2 - zoom_width)
        if zoom_y2 - zoom_y1 < zoom_height:
            zoom_y1 = max(0, zoom_y2 - zoom_height)

        return (zoom_x1, zoom_y1, zoom_x2, zoom_y2)

    def smooth_transition(self, current_region, target_region, alpha=0.1):
        if current_region is None:
            return target_region

        smooth_region = []
        for i in range(4):
            smooth_val = int(current_region[i] * (1 - alpha) + target_region[i] * alpha)
            smooth_region.append(smooth_val)

        return tuple(smooth_region)

    def apply_shake_effect(self, zoom_region, shake_intensity=5):
        if zoom_region is None:
            return zoom_region
            
        x1, y1, x2, y2 = zoom_region
        
        shake_x = random.randint(-shake_intensity, shake_intensity)
        shake_y = random.randint(-shake_intensity, shake_intensity)
        
        new_x1 = max(0, min(self.width - (x2 - x1), x1 + shake_x))
        new_y1 = max(0, min(self.height - (y2 - y1), y1 + shake_y))
        new_x2 = new_x1 + (x2 - x1)
        new_y2 = new_y1 + (y2 - y1)
        
        return (new_x1, new_y1, new_x2, new_y2)

    def calculate_gradual_zoom_factor(self, current_frame, start_frame, end_frame, start_zoom=1.0, end_zoom=1.5):
        if current_frame < start_frame:
            return start_zoom
        elif current_frame >= end_frame:
            return end_zoom
        else:
            progress = (current_frame - start_frame) / (end_frame - start_frame)
            eased_progress = 1 - math.cos(progress * math.pi / 2)
            return start_zoom + (end_zoom - start_zoom) * eased_progress

    def apply_zoom_effect(self, frame, zoom_region, zoom_factor=1.5):
        x1, y1, x2, y2 = zoom_region
        cropped = frame[y1:y2, x1:x2]
        zoomed = cv2.resize(cropped, (self.width, self.height), interpolation=cv2.INTER_CUBIC)
        return zoomed

    def process_video(self, zoom_start_frame=None, zoom_duration_frames=None, zoom_factor=1.5, 
                     zoom_type="instant", gradual_start_frame=None, gradual_end_frame=None, 
                     gradual_hold_frames=None, enable_shake=False, shake_intensity=3, shake_start_delay=0.5):

        frame_count = 0
        zoom_region = None
        target_face = None
        shake_start_frame = None
        shake_end_frame = None
        shake_duration_frames = int(0.5 * self.fps)
        
        if zoom_type == "instant":
            print(f"Instant zoom t·ª´ frame {zoom_start_frame} ƒë·∫øn {zoom_start_frame + zoom_duration_frames}")
            if enable_shake:
                shake_start_frame = zoom_start_frame + int(shake_start_delay * self.fps)
                shake_end_frame = shake_start_frame + shake_duration_frames
                print(f"Shake effect t·ª´ frame {shake_start_frame} ƒë·∫øn {shake_end_frame}")
        else:
            gradual_total_end = gradual_end_frame + (gradual_hold_frames or 0)
            print(f"Gradual zoom t·ª´ frame {gradual_start_frame} ƒë·∫øn {gradual_end_frame}")
            print(f"Gi·ªØ m·ª©c zoom ƒë·∫øn frame {gradual_total_end}")
            if enable_shake:
                shake_start_frame = gradual_end_frame + int(shake_start_delay * self.fps)
                shake_end_frame = shake_start_frame + shake_duration_frames
                print(f"Shake effect t·ª´ frame {shake_start_frame} ƒë·∫øn {shake_end_frame}")

        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    break

                faces = self.detect_faces(frame)
                current_zoom_factor = 1.0
                should_zoom = False

                if zoom_type == "instant":
                    if zoom_start_frame <= frame_count < zoom_start_frame + zoom_duration_frames:
                        should_zoom = True
                        current_zoom_factor = zoom_factor
                elif zoom_type == "gradual":
                    if gradual_start_frame <= frame_count <= gradual_end_frame:
                        should_zoom = True
                        current_zoom_factor = self.calculate_gradual_zoom_factor(
                            frame_count, gradual_start_frame, gradual_end_frame, 1.0, zoom_factor
                        )
                    elif gradual_hold_frames and gradual_end_frame < frame_count <= gradual_end_frame + gradual_hold_frames:
                        should_zoom = True
                        current_zoom_factor = zoom_factor

                if should_zoom and current_zoom_factor > 1.0:
                    if faces:
                        current_largest = self.get_largest_face(faces)

                        if target_face is None:
                            target_face = current_largest
                        else:
                            target_area = (target_face[2] - target_face[0]) * (target_face[3] - target_face[1])
                            current_area = (current_largest[2] - current_largest[0]) * (current_largest[3] - current_largest[1])

                            if current_area > target_area * 1.2:
                                target_face = current_largest

                        target_zoom_region = self.calculate_zoom_region(target_face, current_zoom_factor)
                        zoom_region = self.smooth_transition(zoom_region, target_zoom_region, alpha=0.15)

                        if (enable_shake and shake_start_frame and shake_end_frame and 
                            shake_start_frame <= frame_count < shake_end_frame):
                            zoom_region = self.apply_shake_effect(zoom_region, shake_intensity)

                        frame = self.apply_zoom_effect(frame, zoom_region, current_zoom_factor)

                    elif zoom_region is not None:
                        if (enable_shake and shake_start_frame and shake_end_frame and 
                            shake_start_frame <= frame_count < shake_end_frame):
                            zoom_region = self.apply_shake_effect(zoom_region, shake_intensity)
                        
                        frame = self.apply_zoom_effect(frame, zoom_region, current_zoom_factor)
                else:
                    if not should_zoom:
                        target_face = None
                        zoom_region = None

                self.out.write(frame)
                frame_count += 1
                
                if frame_count % 30 == 0:
                    print(f"ƒê√£ x·ª≠ l√Ω: {frame_count}/{self.total_frames} frames")

        except Exception as e:
            print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω video: {e}")
            raise
        finally:
            # ƒê·∫£m b·∫£o gi·∫£i ph√≥ng t√†i nguy√™n
            self.release()

        print("‚úÖ Ho√†n th√†nh x·ª≠ l√Ω video!")

def create_face_zoom_video(input_video, output_video, zoom_type="instant", **kwargs):
    """
    T·∫°o video v·ªõi hi·ªáu ·ª©ng zoom face - version an to√†n
    """
    try:
        with VideoFaceZoom(input_video, output_video) as processor:
            zoom_factor = kwargs.get('zoom_factor', 1.8)
            enable_shake = kwargs.get('enable_shake', False)
            shake_intensity = kwargs.get('shake_intensity', 3)
            shake_start_delay = kwargs.get('shake_start_delay', 0.5)
            
            if zoom_type == "instant":
                zoom_start_time = kwargs.get('zoom_start_time', 0)
                zoom_duration = kwargs.get('zoom_duration', 2)
                
                zoom_start_frame = int(zoom_start_time * processor.fps)
                zoom_duration_frames = int(zoom_duration * processor.fps)
                
                processor.process_video(
                    zoom_start_frame=zoom_start_frame,
                    zoom_duration_frames=zoom_duration_frames,
                    zoom_factor=zoom_factor,
                    zoom_type="instant",
                    enable_shake=enable_shake,
                    shake_intensity=shake_intensity,
                    shake_start_delay=shake_start_delay
                )
                
            elif zoom_type == "gradual":
                gradual_start_time = kwargs.get('gradual_start_time', 0)
                gradual_end_time = kwargs.get('gradual_end_time', 3)
                hold_duration = kwargs.get('hold_duration', 2)
                
                gradual_start_frame = int(gradual_start_time * processor.fps)
                gradual_end_frame = int(gradual_end_time * processor.fps)
                gradual_hold_frames = int(hold_duration * processor.fps)
                
                processor.process_video(
                    zoom_factor=zoom_factor,
                    zoom_type="gradual",
                    gradual_start_frame=gradual_start_frame,
                    gradual_end_frame=gradual_end_frame,
                    gradual_hold_frames=gradual_hold_frames,
                    enable_shake=enable_shake,
                    shake_intensity=shake_intensity,
                    shake_start_delay=shake_start_delay
                )
                
    except Exception as e:
        print(f"‚ùå L·ªói trong create_face_zoom_video: {e}")
        raise

# S·ª≠ d·ª•ng an to√†n
# if __name__ == "__main__":
#     input_video_path = "/content/output33 (2).mp4"
    
#     print("=== √Åp d·ª•ng hi·ªáu ·ª©ng zoom l·∫ßn 1 ===")
#     success1 = safe_create_face_zoom_video(
#         input_video=input_video_path,
#         output_video="output_zoom_step1.mp4",
#         zoom_type="instant",
#         zoom_start_time=1,
#         zoom_duration=4,
#         zoom_factor=1.3,
#         enable_shake=False,
#         shake_intensity=1,
#         shake_start_delay=0.3
#     )
    
#     if success1:
#         print("=== √Åp d·ª•ng hi·ªáu ·ª©ng zoom l·∫ßn 2 ===")
#         success2 = safe_create_face_zoom_video(
#             input_video="output_zoom_step1.mp4",
#             output_video="output_zoom_step2.mp4",
#             zoom_type="instant",
#             zoom_start_time=3,
#             zoom_duration=3,
#             zoom_factor=1.5,
#             enable_shake=False,
#             shake_intensity=2,
#             shake_start_delay=0.2
#         )
        
#         if success2:
#             print("‚úÖ Ho√†n th√†nh c·∫£ 2 hi·ªáu ·ª©ng!")
#         else:
#             print("‚ùå L·ªói ·ªü hi·ªáu ·ª©ng th·ª© 2")
#     else:
#         print("‚ùå L·ªói ·ªü hi·ªáu ·ª©ng th·ª© 1")